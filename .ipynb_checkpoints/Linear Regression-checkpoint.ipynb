{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x10d11f978>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUw4AZmZjygHAzGxMOQCYmY2p/wekW5Oolk4P5wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample 100 points near a line\n",
    "N = 100\n",
    "M, C = .78, 3.07\n",
    "X = np.linspace(-10, 10, N)\n",
    "Y = (M*X + C) + np.random.randn(N)\n",
    "plt.scatter(X, Y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize m and c\n",
    "m = 0\n",
    "c = 0\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 30.520207\n",
      "Loss = 27.735840\n",
      "Loss = 25.312892\n",
      "Loss = 23.203892\n",
      "Loss = 21.367603\n",
      "Loss = 19.768209\n",
      "Loss = 18.374598\n",
      "Loss = 17.159749\n",
      "Loss = 16.100189\n",
      "Loss = 15.175531\n",
      "Loss = 14.368065\n",
      "Loss = 13.662409\n",
      "Loss = 13.045203\n",
      "Loss = 12.504842\n",
      "Loss = 12.031244\n",
      "Loss = 11.615654\n",
      "Loss = 11.250467\n",
      "Loss = 10.929077\n",
      "Loss = 10.645744\n",
      "Loss = 10.395486\n",
      "Loss = 10.173972\n",
      "Loss = 9.977443\n",
      "Loss = 9.802631\n",
      "Loss = 9.646699\n",
      "Loss = 9.507183\n",
      "Loss = 9.381943\n",
      "Loss = 9.269117\n",
      "Loss = 9.167092\n",
      "Loss = 9.074465\n",
      "Loss = 8.990015\n",
      "Loss = 8.912685\n",
      "Loss = 8.841556\n",
      "Loss = 8.775827\n",
      "Loss = 8.714806\n",
      "Loss = 8.657889\n",
      "Loss = 8.604552\n",
      "Loss = 8.554342\n",
      "Loss = 8.506862\n",
      "Loss = 8.461769\n",
      "Loss = 8.418765\n",
      "Loss = 8.377591\n",
      "Loss = 8.338022\n",
      "Loss = 8.299862\n",
      "Loss = 8.262940\n",
      "Loss = 8.227110\n",
      "Loss = 8.192243\n",
      "Loss = 8.158227\n",
      "Loss = 8.124965\n",
      "Loss = 8.092373\n",
      "Loss = 8.060379\n",
      "Loss = 8.028917\n",
      "Loss = 7.997933\n",
      "Loss = 7.967379\n",
      "Loss = 7.937212\n",
      "Loss = 7.907397\n",
      "Loss = 7.877901\n",
      "Loss = 7.848697\n",
      "Loss = 7.819761\n",
      "Loss = 7.791072\n",
      "Loss = 7.762612\n",
      "Loss = 7.734364\n",
      "Loss = 7.706316\n",
      "Loss = 7.678454\n",
      "Loss = 7.650768\n",
      "Loss = 7.623250\n",
      "Loss = 7.595890\n",
      "Loss = 7.568683\n",
      "Loss = 7.541621\n",
      "Loss = 7.514699\n",
      "Loss = 7.487912\n",
      "Loss = 7.461257\n",
      "Loss = 7.434730\n",
      "Loss = 7.408327\n",
      "Loss = 7.382045\n",
      "Loss = 7.355882\n",
      "Loss = 7.329835\n",
      "Loss = 7.303903\n",
      "Loss = 7.278084\n",
      "Loss = 7.252375\n",
      "Loss = 7.226777\n",
      "Loss = 7.201286\n",
      "Loss = 7.175902\n",
      "Loss = 7.150625\n",
      "Loss = 7.125452\n",
      "Loss = 7.100383\n",
      "Loss = 7.075417\n",
      "Loss = 7.050554\n",
      "Loss = 7.025792\n",
      "Loss = 7.001131\n",
      "Loss = 6.976570\n",
      "Loss = 6.952109\n",
      "Loss = 6.927747\n",
      "Loss = 6.903483\n",
      "Loss = 6.879317\n",
      "Loss = 6.855248\n",
      "Loss = 6.831277\n",
      "Loss = 6.807402\n",
      "Loss = 6.783622\n",
      "Loss = 6.759939\n",
      "Loss = 6.736350\n",
      "Loss = 6.712856\n",
      "Loss = 6.689456\n",
      "Loss = 6.666150\n",
      "Loss = 6.642937\n",
      "Loss = 6.619817\n",
      "Loss = 6.596790\n",
      "Loss = 6.573855\n",
      "Loss = 6.551012\n",
      "Loss = 6.528260\n",
      "Loss = 6.505599\n",
      "Loss = 6.483029\n",
      "Loss = 6.460549\n",
      "Loss = 6.438159\n",
      "Loss = 6.415858\n",
      "Loss = 6.393647\n",
      "Loss = 6.371524\n",
      "Loss = 6.349490\n",
      "Loss = 6.327544\n",
      "Loss = 6.305686\n",
      "Loss = 6.283915\n",
      "Loss = 6.262231\n",
      "Loss = 6.240634\n",
      "Loss = 6.219123\n",
      "Loss = 6.197698\n",
      "Loss = 6.176358\n",
      "Loss = 6.155104\n",
      "Loss = 6.133935\n",
      "Loss = 6.112851\n",
      "Loss = 6.091850\n",
      "Loss = 6.070934\n",
      "Loss = 6.050101\n",
      "Loss = 6.029352\n",
      "Loss = 6.008685\n",
      "Loss = 5.988101\n",
      "Loss = 5.967599\n",
      "Loss = 5.947180\n",
      "Loss = 5.926841\n",
      "Loss = 5.906584\n",
      "Loss = 5.886408\n",
      "Loss = 5.866313\n",
      "Loss = 5.846298\n",
      "Loss = 5.826363\n",
      "Loss = 5.806507\n",
      "Loss = 5.786731\n",
      "Loss = 5.767034\n",
      "Loss = 5.747416\n",
      "Loss = 5.727876\n",
      "Loss = 5.708414\n",
      "Loss = 5.689030\n",
      "Loss = 5.669723\n",
      "Loss = 5.650494\n",
      "Loss = 5.631341\n",
      "Loss = 5.612265\n",
      "Loss = 5.593265\n",
      "Loss = 5.574341\n",
      "Loss = 5.555493\n",
      "Loss = 5.536720\n",
      "Loss = 5.518022\n",
      "Loss = 5.499399\n",
      "Loss = 5.480850\n",
      "Loss = 5.462375\n",
      "Loss = 5.443974\n",
      "Loss = 5.425647\n",
      "Loss = 5.407392\n",
      "Loss = 5.389211\n",
      "Loss = 5.371103\n",
      "Loss = 5.353066\n",
      "Loss = 5.335102\n",
      "Loss = 5.317210\n",
      "Loss = 5.299389\n",
      "Loss = 5.281640\n",
      "Loss = 5.263961\n",
      "Loss = 5.246353\n",
      "Loss = 5.228815\n",
      "Loss = 5.211347\n",
      "Loss = 5.193950\n",
      "Loss = 5.176621\n",
      "Loss = 5.159362\n",
      "Loss = 5.142172\n",
      "Loss = 5.125051\n",
      "Loss = 5.107998\n",
      "Loss = 5.091013\n",
      "Loss = 5.074096\n",
      "Loss = 5.057247\n",
      "Loss = 5.040465\n",
      "Loss = 5.023750\n",
      "Loss = 5.007101\n",
      "Loss = 4.990520\n",
      "Loss = 4.974004\n",
      "Loss = 4.957555\n",
      "Loss = 4.941171\n",
      "Loss = 4.924853\n",
      "Loss = 4.908600\n",
      "Loss = 4.892412\n",
      "Loss = 4.876289\n",
      "Loss = 4.860230\n",
      "Loss = 4.844235\n",
      "Loss = 4.828304\n",
      "Loss = 4.812437\n",
      "Loss = 4.796633\n",
      "Loss = 4.780893\n",
      "Loss = 4.765215\n",
      "Loss = 4.749600\n",
      "Loss = 4.734047\n",
      "Loss = 4.718557\n",
      "Loss = 4.703128\n",
      "Loss = 4.687761\n",
      "Loss = 4.672455\n",
      "Loss = 4.657211\n",
      "Loss = 4.642027\n",
      "Loss = 4.626905\n",
      "Loss = 4.611842\n",
      "Loss = 4.596840\n",
      "Loss = 4.581898\n",
      "Loss = 4.567015\n",
      "Loss = 4.552192\n",
      "Loss = 4.537428\n",
      "Loss = 4.522724\n",
      "Loss = 4.508077\n",
      "Loss = 4.493490\n",
      "Loss = 4.478961\n",
      "Loss = 4.464489\n",
      "Loss = 4.450076\n",
      "Loss = 4.435720\n",
      "Loss = 4.421422\n",
      "Loss = 4.407181\n",
      "Loss = 4.392996\n",
      "Loss = 4.378869\n",
      "Loss = 4.364797\n",
      "Loss = 4.350782\n",
      "Loss = 4.336823\n",
      "Loss = 4.322920\n",
      "Loss = 4.309072\n",
      "Loss = 4.295280\n",
      "Loss = 4.281543\n",
      "Loss = 4.267861\n",
      "Loss = 4.254233\n",
      "Loss = 4.240660\n",
      "Loss = 4.227141\n",
      "Loss = 4.213676\n",
      "Loss = 4.200265\n",
      "Loss = 4.186907\n",
      "Loss = 4.173603\n",
      "Loss = 4.160352\n",
      "Loss = 4.147154\n",
      "Loss = 4.134009\n",
      "Loss = 4.120916\n",
      "Loss = 4.107875\n",
      "Loss = 4.094887\n",
      "Loss = 4.081951\n",
      "Loss = 4.069066\n",
      "Loss = 4.056233\n",
      "Loss = 4.043451\n",
      "Loss = 4.030720\n",
      "Loss = 4.018040\n",
      "Loss = 4.005410\n",
      "Loss = 3.992831\n",
      "Loss = 3.980303\n",
      "Loss = 3.967824\n",
      "Loss = 3.955395\n",
      "Loss = 3.943016\n",
      "Loss = 3.930687\n",
      "Loss = 3.918407\n",
      "Loss = 3.906175\n",
      "Loss = 3.893993\n",
      "Loss = 3.881859\n",
      "Loss = 3.869774\n",
      "Loss = 3.857737\n",
      "Loss = 3.845748\n",
      "Loss = 3.833808\n",
      "Loss = 3.821914\n",
      "Loss = 3.810069\n",
      "Loss = 3.798270\n",
      "Loss = 3.786519\n",
      "Loss = 3.774815\n",
      "Loss = 3.763158\n",
      "Loss = 3.751547\n",
      "Loss = 3.739982\n",
      "Loss = 3.728464\n",
      "Loss = 3.716992\n",
      "Loss = 3.705565\n",
      "Loss = 3.694185\n",
      "Loss = 3.682849\n",
      "Loss = 3.671560\n",
      "Loss = 3.660315\n",
      "Loss = 3.649115\n",
      "Loss = 3.637960\n",
      "Loss = 3.626849\n",
      "Loss = 3.615783\n",
      "Loss = 3.604761\n",
      "Loss = 3.593783\n",
      "Loss = 3.582849\n",
      "Loss = 3.571959\n",
      "Loss = 3.561112\n",
      "Loss = 3.550308\n",
      "Loss = 3.539548\n",
      "Loss = 3.528831\n",
      "Loss = 3.518156\n",
      "Loss = 3.507524\n",
      "Loss = 3.496935\n",
      "Loss = 3.486388\n",
      "Loss = 3.475883\n",
      "Loss = 3.465420\n",
      "Loss = 3.454999\n",
      "Loss = 3.444620\n",
      "Loss = 3.434282\n",
      "Loss = 3.423985\n",
      "Loss = 3.413730\n",
      "Loss = 3.403515\n",
      "Loss = 3.393341\n",
      "Loss = 3.383208\n",
      "Loss = 3.373116\n",
      "Loss = 3.363063\n",
      "Loss = 3.353051\n",
      "Loss = 3.343079\n",
      "Loss = 3.333147\n",
      "Loss = 3.323254\n",
      "Loss = 3.313401\n",
      "Loss = 3.303588\n",
      "Loss = 3.293813\n",
      "Loss = 3.284078\n",
      "Loss = 3.274382\n",
      "Loss = 3.264724\n",
      "Loss = 3.255105\n",
      "Loss = 3.245524\n",
      "Loss = 3.235982\n",
      "Loss = 3.226477\n",
      "Loss = 3.217011\n",
      "Loss = 3.207583\n",
      "Loss = 3.198192\n",
      "Loss = 3.188839\n",
      "Loss = 3.179523\n",
      "Loss = 3.170244\n",
      "Loss = 3.161003\n",
      "Loss = 3.151798\n",
      "Loss = 3.142630\n",
      "Loss = 3.133499\n",
      "Loss = 3.124404\n",
      "Loss = 3.115346\n",
      "Loss = 3.106324\n",
      "Loss = 3.097337\n",
      "Loss = 3.088387\n",
      "Loss = 3.079473\n",
      "Loss = 3.070594\n",
      "Loss = 3.061750\n",
      "Loss = 3.052942\n",
      "Loss = 3.044170\n",
      "Loss = 3.035432\n",
      "Loss = 3.026729\n",
      "Loss = 3.018061\n",
      "Loss = 3.009427\n",
      "Loss = 3.000828\n",
      "Loss = 2.992264\n",
      "Loss = 2.983734\n",
      "Loss = 2.975237\n",
      "Loss = 2.966775\n",
      "Loss = 2.958346\n",
      "Loss = 2.949952\n",
      "Loss = 2.941590\n",
      "Loss = 2.933262\n",
      "Loss = 2.924968\n",
      "Loss = 2.916706\n",
      "Loss = 2.908478\n",
      "Loss = 2.900282\n",
      "Loss = 2.892120\n",
      "Loss = 2.883989\n",
      "Loss = 2.875892\n",
      "Loss = 2.867826\n",
      "Loss = 2.859793\n",
      "Loss = 2.851792\n",
      "Loss = 2.843823\n",
      "Loss = 2.835886\n",
      "Loss = 2.827980\n",
      "Loss = 2.820107\n",
      "Loss = 2.812264\n",
      "Loss = 2.804453\n",
      "Loss = 2.796673\n",
      "Loss = 2.788924\n",
      "Loss = 2.781207\n",
      "Loss = 2.773520\n",
      "Loss = 2.765863\n",
      "Loss = 2.758238\n",
      "Loss = 2.750643\n",
      "Loss = 2.743078\n",
      "Loss = 2.735543\n",
      "Loss = 2.728039\n",
      "Loss = 2.720564\n",
      "Loss = 2.713119\n",
      "Loss = 2.705704\n",
      "Loss = 2.698319\n",
      "Loss = 2.690963\n",
      "Loss = 2.683637\n",
      "Loss = 2.676340\n",
      "Loss = 2.669072\n",
      "Loss = 2.661833\n",
      "Loss = 2.654623\n",
      "Loss = 2.647442\n",
      "Loss = 2.640290\n",
      "Loss = 2.633166\n",
      "Loss = 2.626070\n",
      "Loss = 2.619003\n",
      "Loss = 2.611964\n",
      "Loss = 2.604954\n",
      "Loss = 2.597971\n",
      "Loss = 2.591016\n",
      "Loss = 2.584089\n",
      "Loss = 2.577190\n",
      "Loss = 2.570318\n",
      "Loss = 2.563474\n",
      "Loss = 2.556657\n",
      "Loss = 2.549867\n",
      "Loss = 2.543105\n",
      "Loss = 2.536369\n",
      "Loss = 2.529661\n",
      "Loss = 2.522979\n",
      "Loss = 2.516324\n",
      "Loss = 2.509695\n",
      "Loss = 2.503093\n",
      "Loss = 2.496518\n",
      "Loss = 2.489968\n",
      "Loss = 2.483445\n",
      "Loss = 2.476948\n",
      "Loss = 2.470477\n",
      "Loss = 2.464032\n",
      "Loss = 2.457612\n",
      "Loss = 2.451218\n",
      "Loss = 2.444850\n",
      "Loss = 2.438507\n",
      "Loss = 2.432189\n",
      "Loss = 2.425897\n",
      "Loss = 2.419630\n",
      "Loss = 2.413388\n",
      "Loss = 2.407171\n",
      "Loss = 2.400978\n",
      "Loss = 2.394811\n",
      "Loss = 2.388668\n",
      "Loss = 2.382549\n",
      "Loss = 2.376455\n",
      "Loss = 2.370386\n",
      "Loss = 2.364340\n",
      "Loss = 2.358319\n",
      "Loss = 2.352322\n",
      "Loss = 2.346349\n",
      "Loss = 2.340400\n",
      "Loss = 2.334474\n",
      "Loss = 2.328572\n",
      "Loss = 2.322694\n",
      "Loss = 2.316839\n",
      "Loss = 2.311008\n",
      "Loss = 2.305200\n",
      "Loss = 2.299415\n",
      "Loss = 2.293653\n",
      "Loss = 2.287915\n",
      "Loss = 2.282199\n",
      "Loss = 2.276506\n",
      "Loss = 2.270836\n",
      "Loss = 2.265188\n",
      "Loss = 2.259563\n",
      "Loss = 2.253961\n",
      "Loss = 2.248380\n",
      "Loss = 2.242823\n",
      "Loss = 2.237287\n",
      "Loss = 2.231774\n",
      "Loss = 2.226282\n",
      "Loss = 2.220813\n",
      "Loss = 2.215365\n",
      "Loss = 2.209939\n",
      "Loss = 2.204535\n",
      "Loss = 2.199152\n",
      "Loss = 2.193791\n",
      "Loss = 2.188452\n",
      "Loss = 2.183133\n",
      "Loss = 2.177836\n",
      "Loss = 2.172560\n",
      "Loss = 2.167305\n",
      "Loss = 2.162072\n",
      "Loss = 2.156859\n",
      "Loss = 2.151667\n",
      "Loss = 2.146495\n",
      "Loss = 2.141345\n",
      "Loss = 2.136215\n",
      "Loss = 2.131105\n",
      "Loss = 2.126016\n",
      "Loss = 2.120947\n",
      "Loss = 2.115898\n",
      "Loss = 2.110870\n",
      "Loss = 2.105862\n",
      "Loss = 2.100873\n",
      "Loss = 2.095905\n",
      "Loss = 2.090956\n",
      "Loss = 2.086028\n",
      "Loss = 2.081119\n",
      "Loss = 2.076229\n",
      "Loss = 2.071359\n",
      "Loss = 2.066509\n",
      "Loss = 2.061678\n",
      "Loss = 2.056866\n",
      "Loss = 2.052074\n",
      "Loss = 2.047300\n",
      "Loss = 2.042546\n",
      "Loss = 2.037811\n",
      "Loss = 2.033094\n",
      "Loss = 2.028397\n",
      "Loss = 2.023718\n",
      "Loss = 2.019058\n",
      "Loss = 2.014417\n",
      "Loss = 2.009794\n",
      "Loss = 2.005189\n",
      "Loss = 2.000603\n",
      "Loss = 1.996036\n",
      "Loss = 1.991486\n",
      "Loss = 1.986955\n",
      "Loss = 1.982442\n",
      "Loss = 1.977947\n",
      "Loss = 1.973469\n",
      "Loss = 1.969010\n",
      "Loss = 1.964569\n",
      "Loss = 1.960145\n",
      "Loss = 1.955739\n",
      "Loss = 1.951351\n",
      "Loss = 1.946980\n",
      "Loss = 1.942626\n",
      "Loss = 1.938290\n",
      "Loss = 1.933972\n",
      "Loss = 1.929670\n",
      "Loss = 1.925386\n",
      "Loss = 1.921119\n",
      "Loss = 1.916869\n",
      "Loss = 1.912636\n",
      "Loss = 1.908419\n",
      "Loss = 1.904220\n",
      "Loss = 1.900038\n",
      "Loss = 1.895872\n",
      "Loss = 1.891723\n",
      "Loss = 1.887590\n",
      "Loss = 1.883474\n",
      "Loss = 1.879374\n",
      "Loss = 1.875291\n",
      "Loss = 1.871224\n",
      "Loss = 1.867173\n",
      "Loss = 1.863139\n",
      "Loss = 1.859120\n",
      "Loss = 1.855118\n",
      "Loss = 1.851132\n",
      "Loss = 1.847161\n",
      "Loss = 1.843207\n",
      "Loss = 1.839268\n",
      "Loss = 1.835345\n",
      "Loss = 1.831438\n",
      "Loss = 1.827546\n",
      "Loss = 1.823670\n",
      "Loss = 1.819809\n",
      "Loss = 1.815964\n",
      "Loss = 1.812134\n",
      "Loss = 1.808320\n",
      "Loss = 1.804520\n",
      "Loss = 1.800736\n",
      "Loss = 1.796967\n",
      "Loss = 1.793213\n",
      "Loss = 1.789474\n",
      "Loss = 1.785750\n",
      "Loss = 1.782041\n",
      "Loss = 1.778347\n",
      "Loss = 1.774667\n",
      "Loss = 1.771002\n",
      "Loss = 1.767352\n",
      "Loss = 1.763717\n",
      "Loss = 1.760095\n",
      "Loss = 1.756489\n",
      "Loss = 1.752897\n",
      "Loss = 1.749319\n",
      "Loss = 1.745755\n",
      "Loss = 1.742206\n",
      "Loss = 1.738671\n",
      "Loss = 1.735150\n",
      "Loss = 1.731643\n",
      "Loss = 1.728150\n",
      "Loss = 1.724671\n",
      "Loss = 1.721206\n",
      "Loss = 1.717755\n",
      "Loss = 1.714317\n",
      "Loss = 1.710894\n",
      "Loss = 1.707484\n",
      "Loss = 1.704087\n",
      "Loss = 1.700704\n",
      "Loss = 1.697335\n",
      "Loss = 1.693979\n",
      "Loss = 1.690637\n",
      "Loss = 1.687308\n",
      "Loss = 1.683992\n",
      "Loss = 1.680689\n",
      "Loss = 1.677400\n",
      "Loss = 1.674124\n",
      "Loss = 1.670861\n",
      "Loss = 1.667611\n",
      "Loss = 1.664374\n",
      "Loss = 1.661150\n",
      "Loss = 1.657939\n",
      "Loss = 1.654740\n",
      "Loss = 1.651554\n",
      "Loss = 1.648382\n",
      "Loss = 1.645221\n",
      "Loss = 1.642074\n",
      "Loss = 1.638939\n",
      "Loss = 1.635816\n",
      "Loss = 1.632706\n",
      "Loss = 1.629609\n",
      "Loss = 1.626524\n",
      "Loss = 1.623451\n",
      "Loss = 1.620390\n",
      "Loss = 1.617342\n",
      "Loss = 1.614306\n",
      "Loss = 1.611282\n",
      "Loss = 1.608270\n",
      "Loss = 1.605270\n",
      "Loss = 1.602282\n",
      "Loss = 1.599306\n",
      "Loss = 1.596342\n",
      "Loss = 1.593389\n",
      "Loss = 1.590449\n",
      "Loss = 1.587520\n",
      "Loss = 1.584603\n",
      "Loss = 1.581698\n",
      "Loss = 1.578804\n",
      "Loss = 1.575922\n",
      "Loss = 1.573051\n",
      "Loss = 1.570192\n",
      "Loss = 1.567344\n",
      "Loss = 1.564508\n",
      "Loss = 1.561683\n",
      "Loss = 1.558869\n",
      "Loss = 1.556067\n",
      "Loss = 1.553275\n",
      "Loss = 1.550495\n",
      "Loss = 1.547726\n",
      "Loss = 1.544968\n",
      "Loss = 1.542221\n",
      "Loss = 1.539485\n",
      "Loss = 1.536760\n",
      "Loss = 1.534046\n",
      "Loss = 1.531342\n",
      "Loss = 1.528650\n",
      "Loss = 1.525968\n",
      "Loss = 1.523297\n",
      "Loss = 1.520637\n",
      "Loss = 1.517987\n",
      "Loss = 1.515348\n",
      "Loss = 1.512719\n",
      "Loss = 1.510101\n",
      "Loss = 1.507494\n",
      "Loss = 1.504896\n",
      "Loss = 1.502309\n",
      "Loss = 1.499733\n",
      "Loss = 1.497167\n",
      "Loss = 1.494611\n",
      "Loss = 1.492065\n",
      "Loss = 1.489529\n",
      "Loss = 1.487004\n",
      "Loss = 1.484489\n",
      "Loss = 1.481983\n",
      "Loss = 1.479488\n",
      "Loss = 1.477003\n",
      "Loss = 1.474527\n",
      "Loss = 1.472062\n",
      "Loss = 1.469606\n",
      "Loss = 1.467160\n",
      "Loss = 1.464724\n",
      "Loss = 1.462298\n",
      "Loss = 1.459881\n",
      "Loss = 1.457475\n",
      "Loss = 1.455077\n",
      "Loss = 1.452689\n",
      "Loss = 1.450311\n",
      "Loss = 1.447942\n",
      "Loss = 1.445583\n",
      "Loss = 1.443233\n",
      "Loss = 1.440893\n",
      "Loss = 1.438562\n",
      "Loss = 1.436240\n",
      "Loss = 1.433928\n",
      "Loss = 1.431624\n",
      "Loss = 1.429330\n",
      "Loss = 1.427045\n",
      "Loss = 1.424770\n",
      "Loss = 1.422503\n",
      "Loss = 1.420245\n",
      "Loss = 1.417997\n",
      "Loss = 1.415757\n",
      "Loss = 1.413527\n",
      "Loss = 1.411305\n",
      "Loss = 1.409092\n",
      "Loss = 1.406888\n",
      "Loss = 1.404693\n",
      "Loss = 1.402506\n",
      "Loss = 1.400329\n",
      "Loss = 1.398160\n",
      "Loss = 1.395999\n",
      "Loss = 1.393848\n",
      "Loss = 1.391705\n",
      "Loss = 1.389570\n",
      "Loss = 1.387444\n",
      "Loss = 1.385327\n",
      "Loss = 1.383217\n",
      "Loss = 1.381117\n",
      "Loss = 1.379025\n",
      "Loss = 1.376941\n",
      "Loss = 1.374865\n",
      "Loss = 1.372798\n",
      "Loss = 1.370739\n",
      "Loss = 1.368688\n",
      "Loss = 1.366646\n",
      "Loss = 1.364611\n",
      "Loss = 1.362585\n",
      "Loss = 1.360567\n",
      "Loss = 1.358557\n",
      "Loss = 1.356555\n",
      "Loss = 1.354561\n",
      "Loss = 1.352575\n",
      "Loss = 1.350596\n",
      "Loss = 1.348626\n",
      "Loss = 1.346664\n",
      "Loss = 1.344709\n",
      "Loss = 1.342762\n",
      "Loss = 1.340823\n",
      "Loss = 1.338892\n",
      "Loss = 1.336969\n",
      "Loss = 1.335053\n",
      "Loss = 1.333145\n",
      "Loss = 1.331244\n",
      "Loss = 1.329351\n",
      "Loss = 1.327466\n",
      "Loss = 1.325588\n",
      "Loss = 1.323718\n",
      "Loss = 1.321855\n",
      "Loss = 1.320000\n",
      "Loss = 1.318152\n",
      "Loss = 1.316311\n",
      "Loss = 1.314478\n",
      "Loss = 1.312652\n",
      "Loss = 1.310833\n",
      "Loss = 1.309022\n",
      "Loss = 1.307218\n",
      "Loss = 1.305421\n",
      "Loss = 1.303631\n",
      "Loss = 1.301848\n",
      "Loss = 1.300073\n",
      "Loss = 1.298305\n",
      "Loss = 1.296543\n",
      "Loss = 1.294789\n",
      "Loss = 1.293042\n",
      "Loss = 1.291301\n",
      "Loss = 1.289568\n",
      "Loss = 1.287842\n",
      "Loss = 1.286122\n",
      "Loss = 1.284410\n",
      "Loss = 1.282704\n",
      "Loss = 1.281005\n",
      "Loss = 1.279313\n",
      "Loss = 1.277627\n",
      "Loss = 1.275949\n",
      "Loss = 1.274277\n",
      "Loss = 1.272611\n",
      "Loss = 1.270953\n",
      "Loss = 1.269301\n",
      "Loss = 1.267655\n",
      "Loss = 1.266017\n",
      "Loss = 1.264384\n",
      "Loss = 1.262758\n",
      "Loss = 1.261139\n",
      "Loss = 1.259526\n",
      "Loss = 1.257920\n",
      "Loss = 1.256320\n",
      "Loss = 1.254727\n",
      "Loss = 1.253139\n",
      "Loss = 1.251559\n",
      "Loss = 1.249984\n",
      "Loss = 1.248416\n",
      "Loss = 1.246854\n",
      "Loss = 1.245298\n",
      "Loss = 1.243749\n",
      "Loss = 1.242205\n",
      "Loss = 1.240668\n",
      "Loss = 1.239137\n",
      "Loss = 1.237612\n",
      "Loss = 1.236094\n",
      "Loss = 1.234581\n",
      "Loss = 1.233074\n",
      "Loss = 1.231574\n",
      "Loss = 1.230079\n",
      "Loss = 1.228590\n",
      "Loss = 1.227108\n",
      "Loss = 1.225631\n",
      "Loss = 1.224160\n",
      "Loss = 1.222695\n",
      "Loss = 1.221236\n",
      "Loss = 1.219782\n",
      "Loss = 1.218335\n",
      "Loss = 1.216893\n",
      "Loss = 1.215457\n",
      "Loss = 1.214027\n",
      "Loss = 1.212602\n",
      "Loss = 1.211184\n",
      "Loss = 1.209770\n",
      "Loss = 1.208363\n",
      "Loss = 1.206961\n",
      "Loss = 1.205565\n",
      "Loss = 1.204174\n",
      "Loss = 1.202789\n",
      "Loss = 1.201409\n",
      "Loss = 1.200035\n",
      "Loss = 1.198666\n",
      "Loss = 1.197303\n",
      "Loss = 1.195946\n",
      "Loss = 1.194593\n",
      "Loss = 1.193246\n",
      "Loss = 1.191905\n",
      "Loss = 1.190569\n",
      "Loss = 1.189238\n",
      "Loss = 1.187913\n",
      "Loss = 1.186592\n",
      "Loss = 1.185277\n",
      "Loss = 1.183968\n",
      "Loss = 1.182663\n",
      "Loss = 1.181364\n",
      "Loss = 1.180070\n",
      "Loss = 1.178781\n",
      "Loss = 1.177498\n",
      "Loss = 1.176219\n",
      "Loss = 1.174946\n",
      "Loss = 1.173677\n",
      "Loss = 1.172414\n",
      "Loss = 1.171156\n",
      "Loss = 1.169903\n",
      "Loss = 1.168654\n",
      "Loss = 1.167411\n",
      "Loss = 1.166173\n",
      "Loss = 1.164940\n",
      "Loss = 1.163711\n",
      "Loss = 1.162488\n",
      "Loss = 1.161269\n",
      "Loss = 1.160055\n",
      "Loss = 1.158847\n",
      "Loss = 1.157643\n",
      "Loss = 1.156443\n",
      "Loss = 1.155249\n",
      "Loss = 1.154059\n",
      "Loss = 1.152874\n",
      "Loss = 1.151694\n",
      "Loss = 1.150519\n",
      "Loss = 1.149348\n",
      "Loss = 1.148182\n",
      "Loss = 1.147021\n",
      "Loss = 1.145864\n",
      "Loss = 1.144712\n",
      "Loss = 1.143564\n",
      "Loss = 1.142421\n",
      "Loss = 1.141283\n",
      "Loss = 1.140149\n",
      "Loss = 1.139020\n",
      "Loss = 1.137895\n",
      "Loss = 1.136774\n",
      "Loss = 1.135659\n",
      "Loss = 1.134547\n",
      "Loss = 1.133440\n",
      "Loss = 1.132338\n",
      "Loss = 1.131240\n",
      "Loss = 1.130146\n",
      "Loss = 1.129057\n",
      "Loss = 1.127972\n",
      "Loss = 1.126891\n",
      "Loss = 1.125815\n",
      "Loss = 1.124743\n",
      "Loss = 1.123675\n",
      "Loss = 1.122611\n",
      "Loss = 1.121552\n",
      "Loss = 1.120497\n",
      "Loss = 1.119446\n",
      "Loss = 1.118400\n",
      "Loss = 1.117357\n",
      "Loss = 1.116319\n",
      "Loss = 1.115285\n",
      "Loss = 1.114255\n",
      "Loss = 1.113229\n",
      "Loss = 1.112208\n",
      "Loss = 1.111190\n",
      "Loss = 1.110176\n",
      "Loss = 1.109167\n",
      "Loss = 1.108161\n",
      "Loss = 1.107160\n",
      "Loss = 1.106162\n",
      "Loss = 1.105169\n",
      "Loss = 1.104179\n",
      "Loss = 1.103194\n",
      "Loss = 1.102212\n",
      "Loss = 1.101234\n",
      "Loss = 1.100260\n",
      "Loss = 1.099291\n",
      "Loss = 1.098325\n",
      "Loss = 1.097362\n",
      "Loss = 1.096404\n",
      "Loss = 1.095449\n",
      "Loss = 1.094499\n",
      "Loss = 1.093552\n",
      "Loss = 1.092609\n",
      "Loss = 1.091669\n",
      "Loss = 1.090734\n",
      "Loss = 1.089802\n",
      "Loss = 1.088874\n",
      "Loss = 1.087949\n",
      "Loss = 1.087029\n",
      "Loss = 1.086112\n",
      "Loss = 1.085198\n",
      "Loss = 1.084289\n",
      "Loss = 1.083382\n",
      "Loss = 1.082480\n",
      "Loss = 1.081581\n",
      "Loss = 1.080686\n",
      "Loss = 1.079794\n",
      "Loss = 1.078906\n",
      "Loss = 1.078021\n",
      "Loss = 1.077140\n",
      "Loss = 1.076263\n",
      "Loss = 1.075389\n",
      "Loss = 1.074518\n",
      "Loss = 1.073651\n",
      "Loss = 1.072788\n",
      "Loss = 1.071928\n",
      "Loss = 1.071071\n",
      "Loss = 1.070218\n",
      "Loss = 1.069368\n",
      "Loss = 1.068521\n",
      "Loss = 1.067678\n",
      "Loss = 1.066838\n",
      "Loss = 1.066002\n",
      "Loss = 1.065169\n",
      "Loss = 1.064339\n",
      "Loss = 1.063513\n",
      "Loss = 1.062690\n",
      "Loss = 1.061870\n",
      "Loss = 1.061054\n",
      "Loss = 1.060240\n",
      "Loss = 1.059430\n",
      "Loss = 1.058624\n",
      "Loss = 1.057820\n",
      "Loss = 1.057020\n",
      "Loss = 1.056223\n",
      "Loss = 1.055429\n",
      "Loss = 1.054638\n",
      "Loss = 1.053850\n",
      "Loss = 1.053066\n",
      "Loss = 1.052284\n",
      "Loss = 1.051506\n",
      "Loss = 1.050731\n",
      "Loss = 1.049959\n",
      "Loss = 1.049190\n",
      "Loss = 1.048424\n",
      "Loss = 1.047662\n",
      "Loss = 1.046902\n",
      "Loss = 1.046145\n",
      "Loss = 1.045391\n",
      "Loss = 1.044641\n",
      "Loss = 1.043893\n",
      "Loss = 1.043148\n",
      "Loss = 1.042407\n",
      "Loss = 1.041668\n",
      "Loss = 1.040932\n",
      "Loss = 1.040199\n",
      "Loss = 1.039470\n",
      "Loss = 1.038743\n",
      "Loss = 1.038018\n",
      "Loss = 1.037297\n",
      "Loss = 1.036579\n",
      "Loss = 1.035863\n",
      "Loss = 1.035151\n",
      "Loss = 1.034441\n",
      "Loss = 1.033734\n",
      "Loss = 1.033030\n",
      "Loss = 1.032329\n",
      "Loss = 1.031630\n",
      "Loss = 1.030935\n",
      "Loss = 1.030242\n",
      "Loss = 1.029552\n",
      "Loss = 1.028864\n",
      "Loss = 1.028180\n",
      "Loss = 1.027498\n",
      "Loss = 1.026819\n",
      "Loss = 1.026142\n",
      "Loss = 1.025469\n",
      "Loss = 1.024797\n",
      "Loss = 1.024129\n",
      "Loss = 1.023463\n",
      "Loss = 1.022800\n",
      "Loss = 1.022140\n",
      "Loss = 1.021482\n",
      "Loss = 1.020827\n",
      "Loss = 1.020175\n",
      "Loss = 1.019525\n",
      "Loss = 1.018877\n",
      "Loss = 1.018233\n",
      "Loss = 1.017591\n"
     ]
    }
   ],
   "source": [
    "while i<1000:\n",
    "    Y_hat = m*X + c\n",
    "    costs = (Y_hat-Y)**2    \n",
    "    loss = np.sum(costs)/N\n",
    "    print('Loss = %f' %loss)\n",
    "    dY_hat = 2/N*(Y_hat-Y)\n",
    "    dm = np.dot(dY_hat, X)\n",
    "    dc = np.sum(dY_hat)\n",
    "    m-=.001*dm\n",
    "    c-=.001*dc\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original parameters: {m:0.780000, c: 3.070000}\n",
      "Predicted parameters: {m:0.808352, c: 2.641736}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.grid()\n",
    "plt.scatter(X, Y, 10)\n",
    "plt.plot(X, m*X+c, 'r')\n",
    "print(\"Original parameters: {m:%f, c: %f}\" %(M, C))\n",
    "print(\"Predicted parameters: {m:%f, c: %f}\" %(m, c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us make our code modular\n",
    "\n",
    "This is the thinking:\n",
    "\n",
    "Initialize $\\theta$<br>\n",
    "i = 0 <br>\n",
    "while i < 1000:<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;$Y_{hat}, loss = forwardProp(\\theta, X, Y)$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;$\\partial \\theta = backProp(X, Y_{hat}, Y)$<br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;$\\theta = update(\\theta, \\partial \\theta)$<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample(N = 100):\n",
    "    M, C = .78, 3.07\n",
    "    X = np.linspace(-10, 10, N)\n",
    "    Y = (M*X + C) + np.random.randn(N)\n",
    "    return X, Y, M, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_prop(theta, X, Y):\n",
    "    N = X.shape[0]\n",
    "    m, c = theta['m'], theta['c']\n",
    "    Y_hat = m*X + c\n",
    "    costs = (Y_hat-Y)**2\n",
    "    loss = np.sum(costs)/N\n",
    "    return Y_hat, loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def back_prop(X, Y_hat, Y):\n",
    "    N = X.shape[0]\n",
    "    dY_hat = 2/N*(Y_hat-Y)\n",
    "    dm = np.dot(dY_hat, X)\n",
    "    dc = np.sum(dY_hat)\n",
    "    dtheta = {'dm': dm, 'dc': dc}\n",
    "    return dtheta\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update(theta, dtheta, learning_rate=.001):\n",
    "    theta['m']-=learning_rate*dtheta['dm']\n",
    "    theta['c']-=learning_rate*dtheta['dc']\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=31.686772 at 0th iteration\n",
      "Loss=28.730678 at 1th iteration\n",
      "Loss=26.158656 at 2th iteration\n",
      "Loss=23.920256 at 3th iteration\n",
      "Loss=21.971656 at 4th iteration\n",
      "Loss=20.274794 at 5th iteration\n",
      "Loss=18.796609 at 6th iteration\n",
      "Loss=17.508383 at 7th iteration\n",
      "Loss=16.385172 at 8th iteration\n",
      "Loss=15.405311 at 9th iteration\n",
      "Loss=14.549980 at 10th iteration\n",
      "Loss=13.802832 at 11th iteration\n",
      "Loss=13.149669 at 12th iteration\n",
      "Loss=12.578158 at 13th iteration\n",
      "Loss=12.077587 at 14th iteration\n",
      "Loss=11.638650 at 15th iteration\n",
      "Loss=11.253265 at 16th iteration\n",
      "Loss=10.914413 at 17th iteration\n",
      "Loss=10.615994 at 18th iteration\n",
      "Loss=10.352712 at 19th iteration\n",
      "Loss=10.119967 at 20th iteration\n",
      "Loss=9.913762 at 21th iteration\n",
      "Loss=9.730626 at 22th iteration\n",
      "Loss=9.567544 at 23th iteration\n",
      "Loss=9.421897 at 24th iteration\n",
      "Loss=9.291409 at 25th iteration\n",
      "Loss=9.174104 at 26th iteration\n",
      "Loss=9.068267 at 27th iteration\n",
      "Loss=8.972406 at 28th iteration\n",
      "Loss=8.885226 at 29th iteration\n",
      "Loss=8.805602 at 30th iteration\n",
      "Loss=8.732556 at 31th iteration\n",
      "Loss=8.665240 at 32th iteration\n",
      "Loss=8.602916 at 33th iteration\n",
      "Loss=8.544944 at 34th iteration\n",
      "Loss=8.490767 at 35th iteration\n",
      "Loss=8.439902 at 36th iteration\n",
      "Loss=8.391929 at 37th iteration\n",
      "Loss=8.346482 at 38th iteration\n",
      "Loss=8.303246 at 39th iteration\n",
      "Loss=8.261944 at 40th iteration\n",
      "Loss=8.222338 at 41th iteration\n",
      "Loss=8.184219 at 42th iteration\n",
      "Loss=8.147408 at 43th iteration\n",
      "Loss=8.111746 at 44th iteration\n",
      "Loss=8.077098 at 45th iteration\n",
      "Loss=8.043345 at 46th iteration\n",
      "Loss=8.010384 at 47th iteration\n",
      "Loss=7.978125 at 48th iteration\n",
      "Loss=7.946492 at 49th iteration\n",
      "Loss=7.915415 at 50th iteration\n",
      "Loss=7.884837 at 51th iteration\n",
      "Loss=7.854706 at 52th iteration\n",
      "Loss=7.824977 at 53th iteration\n",
      "Loss=7.795613 at 54th iteration\n",
      "Loss=7.766579 at 55th iteration\n",
      "Loss=7.737845 at 56th iteration\n",
      "Loss=7.709388 at 57th iteration\n",
      "Loss=7.681183 at 58th iteration\n",
      "Loss=7.653213 at 59th iteration\n",
      "Loss=7.625459 at 60th iteration\n",
      "Loss=7.597908 at 61th iteration\n",
      "Loss=7.570546 at 62th iteration\n",
      "Loss=7.543363 at 63th iteration\n",
      "Loss=7.516348 at 64th iteration\n",
      "Loss=7.489494 at 65th iteration\n",
      "Loss=7.462791 at 66th iteration\n",
      "Loss=7.436235 at 67th iteration\n",
      "Loss=7.409819 at 68th iteration\n",
      "Loss=7.383538 at 69th iteration\n",
      "Loss=7.357388 at 70th iteration\n",
      "Loss=7.331365 at 71th iteration\n",
      "Loss=7.305465 at 72th iteration\n",
      "Loss=7.279686 at 73th iteration\n",
      "Loss=7.254024 at 74th iteration\n",
      "Loss=7.228478 at 75th iteration\n",
      "Loss=7.203044 at 76th iteration\n",
      "Loss=7.177722 at 77th iteration\n",
      "Loss=7.152509 at 78th iteration\n",
      "Loss=7.127405 at 79th iteration\n",
      "Loss=7.102407 at 80th iteration\n",
      "Loss=7.077514 at 81th iteration\n",
      "Loss=7.052726 at 82th iteration\n",
      "Loss=7.028040 at 83th iteration\n",
      "Loss=7.003457 at 84th iteration\n",
      "Loss=6.978976 at 85th iteration\n",
      "Loss=6.954595 at 86th iteration\n",
      "Loss=6.930313 at 87th iteration\n",
      "Loss=6.906131 at 88th iteration\n",
      "Loss=6.882047 at 89th iteration\n",
      "Loss=6.858061 at 90th iteration\n",
      "Loss=6.834172 at 91th iteration\n",
      "Loss=6.810380 at 92th iteration\n",
      "Loss=6.786683 at 93th iteration\n",
      "Loss=6.763083 at 94th iteration\n",
      "Loss=6.739577 at 95th iteration\n",
      "Loss=6.716166 at 96th iteration\n",
      "Loss=6.692849 at 97th iteration\n",
      "Loss=6.669626 at 98th iteration\n",
      "Loss=6.646496 at 99th iteration\n",
      "Loss=6.623459 at 100th iteration\n",
      "Loss=6.600514 at 101th iteration\n",
      "Loss=6.577661 at 102th iteration\n",
      "Loss=6.554900 at 103th iteration\n",
      "Loss=6.532230 at 104th iteration\n",
      "Loss=6.509650 at 105th iteration\n",
      "Loss=6.487161 at 106th iteration\n",
      "Loss=6.464762 at 107th iteration\n",
      "Loss=6.442453 at 108th iteration\n",
      "Loss=6.420233 at 109th iteration\n",
      "Loss=6.398102 at 110th iteration\n",
      "Loss=6.376059 at 111th iteration\n",
      "Loss=6.354105 at 112th iteration\n",
      "Loss=6.332238 at 113th iteration\n",
      "Loss=6.310459 at 114th iteration\n",
      "Loss=6.288767 at 115th iteration\n",
      "Loss=6.267161 at 116th iteration\n",
      "Loss=6.245642 at 117th iteration\n",
      "Loss=6.224209 at 118th iteration\n",
      "Loss=6.202861 at 119th iteration\n",
      "Loss=6.181599 at 120th iteration\n",
      "Loss=6.160422 at 121th iteration\n",
      "Loss=6.139330 at 122th iteration\n",
      "Loss=6.118322 at 123th iteration\n",
      "Loss=6.097397 at 124th iteration\n",
      "Loss=6.076557 at 125th iteration\n",
      "Loss=6.055799 at 126th iteration\n",
      "Loss=6.035125 at 127th iteration\n",
      "Loss=6.014533 at 128th iteration\n",
      "Loss=5.994024 at 129th iteration\n",
      "Loss=5.973597 at 130th iteration\n",
      "Loss=5.953251 at 131th iteration\n",
      "Loss=5.932986 at 132th iteration\n",
      "Loss=5.912803 at 133th iteration\n",
      "Loss=5.892700 at 134th iteration\n",
      "Loss=5.872677 at 135th iteration\n",
      "Loss=5.852735 at 136th iteration\n",
      "Loss=5.832872 at 137th iteration\n",
      "Loss=5.813088 at 138th iteration\n",
      "Loss=5.793384 at 139th iteration\n",
      "Loss=5.773758 at 140th iteration\n",
      "Loss=5.754211 at 141th iteration\n",
      "Loss=5.734742 at 142th iteration\n",
      "Loss=5.715350 at 143th iteration\n",
      "Loss=5.696037 at 144th iteration\n",
      "Loss=5.676800 at 145th iteration\n",
      "Loss=5.657640 at 146th iteration\n",
      "Loss=5.638557 at 147th iteration\n",
      "Loss=5.619550 at 148th iteration\n",
      "Loss=5.600619 at 149th iteration\n",
      "Loss=5.581763 at 150th iteration\n",
      "Loss=5.562983 at 151th iteration\n",
      "Loss=5.544278 at 152th iteration\n",
      "Loss=5.525648 at 153th iteration\n",
      "Loss=5.507092 at 154th iteration\n",
      "Loss=5.488611 at 155th iteration\n",
      "Loss=5.470203 at 156th iteration\n",
      "Loss=5.451869 at 157th iteration\n",
      "Loss=5.433608 at 158th iteration\n",
      "Loss=5.415420 at 159th iteration\n",
      "Loss=5.397304 at 160th iteration\n",
      "Loss=5.379261 at 161th iteration\n",
      "Loss=5.361291 at 162th iteration\n",
      "Loss=5.343392 at 163th iteration\n",
      "Loss=5.325564 at 164th iteration\n",
      "Loss=5.307808 at 165th iteration\n",
      "Loss=5.290122 at 166th iteration\n",
      "Loss=5.272508 at 167th iteration\n",
      "Loss=5.254964 at 168th iteration\n",
      "Loss=5.237489 at 169th iteration\n",
      "Loss=5.220085 at 170th iteration\n",
      "Loss=5.202750 at 171th iteration\n",
      "Loss=5.185485 at 172th iteration\n",
      "Loss=5.168288 at 173th iteration\n",
      "Loss=5.151161 at 174th iteration\n",
      "Loss=5.134101 at 175th iteration\n",
      "Loss=5.117110 at 176th iteration\n",
      "Loss=5.100187 at 177th iteration\n",
      "Loss=5.083331 at 178th iteration\n",
      "Loss=5.066543 at 179th iteration\n",
      "Loss=5.049822 at 180th iteration\n",
      "Loss=5.033167 at 181th iteration\n",
      "Loss=5.016579 at 182th iteration\n",
      "Loss=5.000058 at 183th iteration\n",
      "Loss=4.983602 at 184th iteration\n",
      "Loss=4.967212 at 185th iteration\n",
      "Loss=4.950888 at 186th iteration\n",
      "Loss=4.934629 at 187th iteration\n",
      "Loss=4.918435 at 188th iteration\n",
      "Loss=4.902306 at 189th iteration\n",
      "Loss=4.886241 at 190th iteration\n",
      "Loss=4.870240 at 191th iteration\n",
      "Loss=4.854303 at 192th iteration\n",
      "Loss=4.838430 at 193th iteration\n",
      "Loss=4.822620 at 194th iteration\n",
      "Loss=4.806874 at 195th iteration\n",
      "Loss=4.791190 at 196th iteration\n",
      "Loss=4.775569 at 197th iteration\n",
      "Loss=4.760011 at 198th iteration\n",
      "Loss=4.744515 at 199th iteration\n",
      "Loss=4.729080 at 200th iteration\n",
      "Loss=4.713707 at 201th iteration\n",
      "Loss=4.698396 at 202th iteration\n",
      "Loss=4.683146 at 203th iteration\n",
      "Loss=4.667957 at 204th iteration\n",
      "Loss=4.652828 at 205th iteration\n",
      "Loss=4.637760 at 206th iteration\n",
      "Loss=4.622753 at 207th iteration\n",
      "Loss=4.607805 at 208th iteration\n",
      "Loss=4.592917 at 209th iteration\n",
      "Loss=4.578088 at 210th iteration\n",
      "Loss=4.563319 at 211th iteration\n",
      "Loss=4.548608 at 212th iteration\n",
      "Loss=4.533957 at 213th iteration\n",
      "Loss=4.519364 at 214th iteration\n",
      "Loss=4.504829 at 215th iteration\n",
      "Loss=4.490353 at 216th iteration\n",
      "Loss=4.475934 at 217th iteration\n",
      "Loss=4.461573 at 218th iteration\n",
      "Loss=4.447269 at 219th iteration\n",
      "Loss=4.433022 at 220th iteration\n",
      "Loss=4.418833 at 221th iteration\n",
      "Loss=4.404700 at 222th iteration\n",
      "Loss=4.390623 at 223th iteration\n",
      "Loss=4.376603 at 224th iteration\n",
      "Loss=4.362639 at 225th iteration\n",
      "Loss=4.348730 at 226th iteration\n",
      "Loss=4.334877 at 227th iteration\n",
      "Loss=4.321080 at 228th iteration\n",
      "Loss=4.307338 at 229th iteration\n",
      "Loss=4.293650 at 230th iteration\n",
      "Loss=4.280017 at 231th iteration\n",
      "Loss=4.266439 at 232th iteration\n",
      "Loss=4.252915 at 233th iteration\n",
      "Loss=4.239445 at 234th iteration\n",
      "Loss=4.226029 at 235th iteration\n",
      "Loss=4.212667 at 236th iteration\n",
      "Loss=4.199357 at 237th iteration\n",
      "Loss=4.186102 at 238th iteration\n",
      "Loss=4.172899 at 239th iteration\n",
      "Loss=4.159748 at 240th iteration\n",
      "Loss=4.146651 at 241th iteration\n",
      "Loss=4.133605 at 242th iteration\n",
      "Loss=4.120612 at 243th iteration\n",
      "Loss=4.107671 at 244th iteration\n",
      "Loss=4.094781 at 245th iteration\n",
      "Loss=4.081943 at 246th iteration\n",
      "Loss=4.069156 at 247th iteration\n",
      "Loss=4.056421 at 248th iteration\n",
      "Loss=4.043736 at 249th iteration\n",
      "Loss=4.031102 at 250th iteration\n",
      "Loss=4.018518 at 251th iteration\n",
      "Loss=4.005985 at 252th iteration\n",
      "Loss=3.993502 at 253th iteration\n",
      "Loss=3.981069 at 254th iteration\n",
      "Loss=3.968685 at 255th iteration\n",
      "Loss=3.956351 at 256th iteration\n",
      "Loss=3.944066 at 257th iteration\n",
      "Loss=3.931830 at 258th iteration\n",
      "Loss=3.919643 at 259th iteration\n",
      "Loss=3.907505 at 260th iteration\n",
      "Loss=3.895415 at 261th iteration\n",
      "Loss=3.883374 at 262th iteration\n",
      "Loss=3.871381 at 263th iteration\n",
      "Loss=3.859435 at 264th iteration\n",
      "Loss=3.847538 at 265th iteration\n",
      "Loss=3.835687 at 266th iteration\n",
      "Loss=3.823885 at 267th iteration\n",
      "Loss=3.812129 at 268th iteration\n",
      "Loss=3.800421 at 269th iteration\n",
      "Loss=3.788759 at 270th iteration\n",
      "Loss=3.777144 at 271th iteration\n",
      "Loss=3.765575 at 272th iteration\n",
      "Loss=3.754052 at 273th iteration\n",
      "Loss=3.742576 at 274th iteration\n",
      "Loss=3.731145 at 275th iteration\n",
      "Loss=3.719760 at 276th iteration\n",
      "Loss=3.708421 at 277th iteration\n",
      "Loss=3.697127 at 278th iteration\n",
      "Loss=3.685878 at 279th iteration\n",
      "Loss=3.674673 at 280th iteration\n",
      "Loss=3.663514 at 281th iteration\n",
      "Loss=3.652399 at 282th iteration\n",
      "Loss=3.641329 at 283th iteration\n",
      "Loss=3.630303 at 284th iteration\n",
      "Loss=3.619321 at 285th iteration\n",
      "Loss=3.608383 at 286th iteration\n",
      "Loss=3.597489 at 287th iteration\n",
      "Loss=3.586638 at 288th iteration\n",
      "Loss=3.575830 at 289th iteration\n",
      "Loss=3.565066 at 290th iteration\n",
      "Loss=3.554345 at 291th iteration\n",
      "Loss=3.543666 at 292th iteration\n",
      "Loss=3.533030 at 293th iteration\n",
      "Loss=3.522437 at 294th iteration\n",
      "Loss=3.511886 at 295th iteration\n",
      "Loss=3.501377 at 296th iteration\n",
      "Loss=3.490910 at 297th iteration\n",
      "Loss=3.480485 at 298th iteration\n",
      "Loss=3.470102 at 299th iteration\n",
      "Loss=3.459760 at 300th iteration\n",
      "Loss=3.449460 at 301th iteration\n",
      "Loss=3.439200 at 302th iteration\n",
      "Loss=3.428982 at 303th iteration\n",
      "Loss=3.418804 at 304th iteration\n",
      "Loss=3.408668 at 305th iteration\n",
      "Loss=3.398571 at 306th iteration\n",
      "Loss=3.388515 at 307th iteration\n",
      "Loss=3.378499 at 308th iteration\n",
      "Loss=3.368524 at 309th iteration\n",
      "Loss=3.358588 at 310th iteration\n",
      "Loss=3.348691 at 311th iteration\n",
      "Loss=3.338835 at 312th iteration\n",
      "Loss=3.329017 at 313th iteration\n",
      "Loss=3.319239 at 314th iteration\n",
      "Loss=3.309500 at 315th iteration\n",
      "Loss=3.299800 at 316th iteration\n",
      "Loss=3.290139 at 317th iteration\n",
      "Loss=3.280516 at 318th iteration\n",
      "Loss=3.270932 at 319th iteration\n",
      "Loss=3.261386 at 320th iteration\n",
      "Loss=3.251878 at 321th iteration\n",
      "Loss=3.242409 at 322th iteration\n",
      "Loss=3.232977 at 323th iteration\n",
      "Loss=3.223582 at 324th iteration\n",
      "Loss=3.214226 at 325th iteration\n",
      "Loss=3.204906 at 326th iteration\n",
      "Loss=3.195624 at 327th iteration\n",
      "Loss=3.186379 at 328th iteration\n",
      "Loss=3.177171 at 329th iteration\n",
      "Loss=3.168000 at 330th iteration\n",
      "Loss=3.158865 at 331th iteration\n",
      "Loss=3.149767 at 332th iteration\n",
      "Loss=3.140705 at 333th iteration\n",
      "Loss=3.131680 at 334th iteration\n",
      "Loss=3.122690 at 335th iteration\n",
      "Loss=3.113736 at 336th iteration\n",
      "Loss=3.104819 at 337th iteration\n",
      "Loss=3.095936 at 338th iteration\n",
      "Loss=3.087090 at 339th iteration\n",
      "Loss=3.078278 at 340th iteration\n",
      "Loss=3.069502 at 341th iteration\n",
      "Loss=3.060761 at 342th iteration\n",
      "Loss=3.052055 at 343th iteration\n",
      "Loss=3.043384 at 344th iteration\n",
      "Loss=3.034747 at 345th iteration\n",
      "Loss=3.026145 at 346th iteration\n",
      "Loss=3.017577 at 347th iteration\n",
      "Loss=3.009044 at 348th iteration\n",
      "Loss=3.000544 at 349th iteration\n",
      "Loss=2.992079 at 350th iteration\n",
      "Loss=2.983647 at 351th iteration\n",
      "Loss=2.975249 at 352th iteration\n",
      "Loss=2.966885 at 353th iteration\n",
      "Loss=2.958554 at 354th iteration\n",
      "Loss=2.950256 at 355th iteration\n",
      "Loss=2.941992 at 356th iteration\n",
      "Loss=2.933760 at 357th iteration\n",
      "Loss=2.925561 at 358th iteration\n",
      "Loss=2.917396 at 359th iteration\n",
      "Loss=2.909262 at 360th iteration\n",
      "Loss=2.901162 at 361th iteration\n",
      "Loss=2.893093 at 362th iteration\n",
      "Loss=2.885057 at 363th iteration\n",
      "Loss=2.877053 at 364th iteration\n",
      "Loss=2.869081 at 365th iteration\n",
      "Loss=2.861141 at 366th iteration\n",
      "Loss=2.853233 at 367th iteration\n",
      "Loss=2.845356 at 368th iteration\n",
      "Loss=2.837510 at 369th iteration\n",
      "Loss=2.829696 at 370th iteration\n",
      "Loss=2.821914 at 371th iteration\n",
      "Loss=2.814162 at 372th iteration\n",
      "Loss=2.806441 at 373th iteration\n",
      "Loss=2.798751 at 374th iteration\n",
      "Loss=2.791092 at 375th iteration\n",
      "Loss=2.783464 at 376th iteration\n",
      "Loss=2.775866 at 377th iteration\n",
      "Loss=2.768298 at 378th iteration\n",
      "Loss=2.760761 at 379th iteration\n",
      "Loss=2.753253 at 380th iteration\n",
      "Loss=2.745776 at 381th iteration\n",
      "Loss=2.738329 at 382th iteration\n",
      "Loss=2.730911 at 383th iteration\n",
      "Loss=2.723523 at 384th iteration\n",
      "Loss=2.716164 at 385th iteration\n",
      "Loss=2.708835 at 386th iteration\n",
      "Loss=2.701535 at 387th iteration\n",
      "Loss=2.694265 at 388th iteration\n",
      "Loss=2.687023 at 389th iteration\n",
      "Loss=2.679810 at 390th iteration\n",
      "Loss=2.672627 at 391th iteration\n",
      "Loss=2.665472 at 392th iteration\n",
      "Loss=2.658345 at 393th iteration\n",
      "Loss=2.651247 at 394th iteration\n",
      "Loss=2.644177 at 395th iteration\n",
      "Loss=2.637136 at 396th iteration\n",
      "Loss=2.630122 at 397th iteration\n",
      "Loss=2.623137 at 398th iteration\n",
      "Loss=2.616180 at 399th iteration\n",
      "Loss=2.609250 at 400th iteration\n",
      "Loss=2.602348 at 401th iteration\n",
      "Loss=2.595474 at 402th iteration\n",
      "Loss=2.588627 at 403th iteration\n",
      "Loss=2.581808 at 404th iteration\n",
      "Loss=2.575016 at 405th iteration\n",
      "Loss=2.568251 at 406th iteration\n",
      "Loss=2.561512 at 407th iteration\n",
      "Loss=2.554801 at 408th iteration\n",
      "Loss=2.548117 at 409th iteration\n",
      "Loss=2.541459 at 410th iteration\n",
      "Loss=2.534829 at 411th iteration\n",
      "Loss=2.528224 at 412th iteration\n",
      "Loss=2.521646 at 413th iteration\n",
      "Loss=2.515094 at 414th iteration\n",
      "Loss=2.508568 at 415th iteration\n",
      "Loss=2.502069 at 416th iteration\n",
      "Loss=2.495595 at 417th iteration\n",
      "Loss=2.489148 at 418th iteration\n",
      "Loss=2.482726 at 419th iteration\n",
      "Loss=2.476329 at 420th iteration\n",
      "Loss=2.469959 at 421th iteration\n",
      "Loss=2.463613 at 422th iteration\n",
      "Loss=2.457294 at 423th iteration\n",
      "Loss=2.450999 at 424th iteration\n",
      "Loss=2.444729 at 425th iteration\n",
      "Loss=2.438485 at 426th iteration\n",
      "Loss=2.432265 at 427th iteration\n",
      "Loss=2.426071 at 428th iteration\n",
      "Loss=2.419901 at 429th iteration\n",
      "Loss=2.413756 at 430th iteration\n",
      "Loss=2.407635 at 431th iteration\n",
      "Loss=2.401539 at 432th iteration\n",
      "Loss=2.395467 at 433th iteration\n",
      "Loss=2.389419 at 434th iteration\n",
      "Loss=2.383396 at 435th iteration\n",
      "Loss=2.377397 at 436th iteration\n",
      "Loss=2.371421 at 437th iteration\n",
      "Loss=2.365470 at 438th iteration\n",
      "Loss=2.359542 at 439th iteration\n",
      "Loss=2.353638 at 440th iteration\n",
      "Loss=2.347757 at 441th iteration\n",
      "Loss=2.341900 at 442th iteration\n",
      "Loss=2.336067 at 443th iteration\n",
      "Loss=2.330257 at 444th iteration\n",
      "Loss=2.324470 at 445th iteration\n",
      "Loss=2.318706 at 446th iteration\n",
      "Loss=2.312965 at 447th iteration\n",
      "Loss=2.307247 at 448th iteration\n",
      "Loss=2.301552 at 449th iteration\n",
      "Loss=2.295880 at 450th iteration\n",
      "Loss=2.290230 at 451th iteration\n",
      "Loss=2.284603 at 452th iteration\n",
      "Loss=2.278998 at 453th iteration\n",
      "Loss=2.273416 at 454th iteration\n",
      "Loss=2.267856 at 455th iteration\n",
      "Loss=2.262319 at 456th iteration\n",
      "Loss=2.256803 at 457th iteration\n",
      "Loss=2.251310 at 458th iteration\n",
      "Loss=2.245838 at 459th iteration\n",
      "Loss=2.240388 at 460th iteration\n",
      "Loss=2.234960 at 461th iteration\n",
      "Loss=2.229554 at 462th iteration\n",
      "Loss=2.224170 at 463th iteration\n",
      "Loss=2.218806 at 464th iteration\n",
      "Loss=2.213465 at 465th iteration\n",
      "Loss=2.208145 at 466th iteration\n",
      "Loss=2.202845 at 467th iteration\n",
      "Loss=2.197568 at 468th iteration\n",
      "Loss=2.192311 at 469th iteration\n",
      "Loss=2.187075 at 470th iteration\n",
      "Loss=2.181860 at 471th iteration\n",
      "Loss=2.176666 at 472th iteration\n",
      "Loss=2.171493 at 473th iteration\n",
      "Loss=2.166340 at 474th iteration\n",
      "Loss=2.161208 at 475th iteration\n",
      "Loss=2.156097 at 476th iteration\n",
      "Loss=2.151006 at 477th iteration\n",
      "Loss=2.145935 at 478th iteration\n",
      "Loss=2.140884 at 479th iteration\n",
      "Loss=2.135854 at 480th iteration\n",
      "Loss=2.130844 at 481th iteration\n",
      "Loss=2.125854 at 482th iteration\n",
      "Loss=2.120884 at 483th iteration\n",
      "Loss=2.115933 at 484th iteration\n",
      "Loss=2.111003 at 485th iteration\n",
      "Loss=2.106092 at 486th iteration\n",
      "Loss=2.101201 at 487th iteration\n",
      "Loss=2.096329 at 488th iteration\n",
      "Loss=2.091477 at 489th iteration\n",
      "Loss=2.086644 at 490th iteration\n",
      "Loss=2.081830 at 491th iteration\n",
      "Loss=2.077036 at 492th iteration\n",
      "Loss=2.072261 at 493th iteration\n",
      "Loss=2.067505 at 494th iteration\n",
      "Loss=2.062768 at 495th iteration\n",
      "Loss=2.058049 at 496th iteration\n",
      "Loss=2.053350 at 497th iteration\n",
      "Loss=2.048670 at 498th iteration\n",
      "Loss=2.044008 at 499th iteration\n",
      "Loss=2.039365 at 500th iteration\n",
      "Loss=2.034740 at 501th iteration\n",
      "Loss=2.030134 at 502th iteration\n",
      "Loss=2.025546 at 503th iteration\n",
      "Loss=2.020977 at 504th iteration\n",
      "Loss=2.016426 at 505th iteration\n",
      "Loss=2.011893 at 506th iteration\n",
      "Loss=2.007378 at 507th iteration\n",
      "Loss=2.002881 at 508th iteration\n",
      "Loss=1.998402 at 509th iteration\n",
      "Loss=1.993941 at 510th iteration\n",
      "Loss=1.989498 at 511th iteration\n",
      "Loss=1.985073 at 512th iteration\n",
      "Loss=1.980665 at 513th iteration\n",
      "Loss=1.976275 at 514th iteration\n",
      "Loss=1.971903 at 515th iteration\n",
      "Loss=1.967548 at 516th iteration\n",
      "Loss=1.963210 at 517th iteration\n",
      "Loss=1.958890 at 518th iteration\n",
      "Loss=1.954587 at 519th iteration\n",
      "Loss=1.950301 at 520th iteration\n",
      "Loss=1.946032 at 521th iteration\n",
      "Loss=1.941781 at 522th iteration\n",
      "Loss=1.937546 at 523th iteration\n",
      "Loss=1.933328 at 524th iteration\n",
      "Loss=1.929127 at 525th iteration\n",
      "Loss=1.924943 at 526th iteration\n",
      "Loss=1.920776 at 527th iteration\n",
      "Loss=1.916625 at 528th iteration\n",
      "Loss=1.912491 at 529th iteration\n",
      "Loss=1.908373 at 530th iteration\n",
      "Loss=1.904272 at 531th iteration\n",
      "Loss=1.900187 at 532th iteration\n",
      "Loss=1.896119 at 533th iteration\n",
      "Loss=1.892067 at 534th iteration\n",
      "Loss=1.888031 at 535th iteration\n",
      "Loss=1.884011 at 536th iteration\n",
      "Loss=1.880007 at 537th iteration\n",
      "Loss=1.876019 at 538th iteration\n",
      "Loss=1.872047 at 539th iteration\n",
      "Loss=1.868091 at 540th iteration\n",
      "Loss=1.864151 at 541th iteration\n",
      "Loss=1.860227 at 542th iteration\n",
      "Loss=1.856318 at 543th iteration\n",
      "Loss=1.852425 at 544th iteration\n",
      "Loss=1.848547 at 545th iteration\n",
      "Loss=1.844685 at 546th iteration\n",
      "Loss=1.840838 at 547th iteration\n",
      "Loss=1.837007 at 548th iteration\n",
      "Loss=1.833191 at 549th iteration\n",
      "Loss=1.829390 at 550th iteration\n",
      "Loss=1.825605 at 551th iteration\n",
      "Loss=1.821834 at 552th iteration\n",
      "Loss=1.818079 at 553th iteration\n",
      "Loss=1.814339 at 554th iteration\n",
      "Loss=1.810613 at 555th iteration\n",
      "Loss=1.806903 at 556th iteration\n",
      "Loss=1.803207 at 557th iteration\n",
      "Loss=1.799526 at 558th iteration\n",
      "Loss=1.795860 at 559th iteration\n",
      "Loss=1.792208 at 560th iteration\n",
      "Loss=1.788571 at 561th iteration\n",
      "Loss=1.784949 at 562th iteration\n",
      "Loss=1.781341 at 563th iteration\n",
      "Loss=1.777747 at 564th iteration\n",
      "Loss=1.774168 at 565th iteration\n",
      "Loss=1.770603 at 566th iteration\n",
      "Loss=1.767053 at 567th iteration\n",
      "Loss=1.763516 at 568th iteration\n",
      "Loss=1.759994 at 569th iteration\n",
      "Loss=1.756486 at 570th iteration\n",
      "Loss=1.752991 at 571th iteration\n",
      "Loss=1.749511 at 572th iteration\n",
      "Loss=1.746045 at 573th iteration\n",
      "Loss=1.742592 at 574th iteration\n",
      "Loss=1.739153 at 575th iteration\n",
      "Loss=1.735728 at 576th iteration\n",
      "Loss=1.732317 at 577th iteration\n",
      "Loss=1.728920 at 578th iteration\n",
      "Loss=1.725535 at 579th iteration\n",
      "Loss=1.722165 at 580th iteration\n",
      "Loss=1.718808 at 581th iteration\n",
      "Loss=1.715464 at 582th iteration\n",
      "Loss=1.712134 at 583th iteration\n",
      "Loss=1.708817 at 584th iteration\n",
      "Loss=1.705513 at 585th iteration\n",
      "Loss=1.702223 at 586th iteration\n",
      "Loss=1.698945 at 587th iteration\n",
      "Loss=1.695681 at 588th iteration\n",
      "Loss=1.692430 at 589th iteration\n",
      "Loss=1.689191 at 590th iteration\n",
      "Loss=1.685966 at 591th iteration\n",
      "Loss=1.682754 at 592th iteration\n",
      "Loss=1.679554 at 593th iteration\n",
      "Loss=1.676367 at 594th iteration\n",
      "Loss=1.673193 at 595th iteration\n",
      "Loss=1.670032 at 596th iteration\n",
      "Loss=1.666883 at 597th iteration\n",
      "Loss=1.663747 at 598th iteration\n",
      "Loss=1.660623 at 599th iteration\n",
      "Loss=1.657512 at 600th iteration\n",
      "Loss=1.654413 at 601th iteration\n",
      "Loss=1.651327 at 602th iteration\n",
      "Loss=1.648253 at 603th iteration\n",
      "Loss=1.645191 at 604th iteration\n",
      "Loss=1.642142 at 605th iteration\n",
      "Loss=1.639104 at 606th iteration\n",
      "Loss=1.636079 at 607th iteration\n",
      "Loss=1.633066 at 608th iteration\n",
      "Loss=1.630065 at 609th iteration\n",
      "Loss=1.627076 at 610th iteration\n",
      "Loss=1.624099 at 611th iteration\n",
      "Loss=1.621134 at 612th iteration\n",
      "Loss=1.618180 at 613th iteration\n",
      "Loss=1.615239 at 614th iteration\n",
      "Loss=1.612309 at 615th iteration\n",
      "Loss=1.609391 at 616th iteration\n",
      "Loss=1.606484 at 617th iteration\n",
      "Loss=1.603590 at 618th iteration\n",
      "Loss=1.600706 at 619th iteration\n",
      "Loss=1.597835 at 620th iteration\n",
      "Loss=1.594974 at 621th iteration\n",
      "Loss=1.592126 at 622th iteration\n",
      "Loss=1.589288 at 623th iteration\n",
      "Loss=1.586462 at 624th iteration\n",
      "Loss=1.583647 at 625th iteration\n",
      "Loss=1.580844 at 626th iteration\n",
      "Loss=1.578051 at 627th iteration\n",
      "Loss=1.575270 at 628th iteration\n",
      "Loss=1.572500 at 629th iteration\n",
      "Loss=1.569741 at 630th iteration\n",
      "Loss=1.566993 at 631th iteration\n",
      "Loss=1.564256 at 632th iteration\n",
      "Loss=1.561530 at 633th iteration\n",
      "Loss=1.558815 at 634th iteration\n",
      "Loss=1.556110 at 635th iteration\n",
      "Loss=1.553417 at 636th iteration\n",
      "Loss=1.550734 at 637th iteration\n",
      "Loss=1.548062 at 638th iteration\n",
      "Loss=1.545401 at 639th iteration\n",
      "Loss=1.542750 at 640th iteration\n",
      "Loss=1.540110 at 641th iteration\n",
      "Loss=1.537480 at 642th iteration\n",
      "Loss=1.534861 at 643th iteration\n",
      "Loss=1.532252 at 644th iteration\n",
      "Loss=1.529654 at 645th iteration\n",
      "Loss=1.527066 at 646th iteration\n",
      "Loss=1.524489 at 647th iteration\n",
      "Loss=1.521922 at 648th iteration\n",
      "Loss=1.519365 at 649th iteration\n",
      "Loss=1.516818 at 650th iteration\n",
      "Loss=1.514282 at 651th iteration\n",
      "Loss=1.511755 at 652th iteration\n",
      "Loss=1.509239 at 653th iteration\n",
      "Loss=1.506733 at 654th iteration\n",
      "Loss=1.504237 at 655th iteration\n",
      "Loss=1.501750 at 656th iteration\n",
      "Loss=1.499274 at 657th iteration\n",
      "Loss=1.496808 at 658th iteration\n",
      "Loss=1.494351 at 659th iteration\n",
      "Loss=1.491904 at 660th iteration\n",
      "Loss=1.489467 at 661th iteration\n",
      "Loss=1.487040 at 662th iteration\n",
      "Loss=1.484623 at 663th iteration\n",
      "Loss=1.482215 at 664th iteration\n",
      "Loss=1.479817 at 665th iteration\n",
      "Loss=1.477428 at 666th iteration\n",
      "Loss=1.475049 at 667th iteration\n",
      "Loss=1.472679 at 668th iteration\n",
      "Loss=1.470319 at 669th iteration\n",
      "Loss=1.467968 at 670th iteration\n",
      "Loss=1.465627 at 671th iteration\n",
      "Loss=1.463295 at 672th iteration\n",
      "Loss=1.460972 at 673th iteration\n",
      "Loss=1.458659 at 674th iteration\n",
      "Loss=1.456355 at 675th iteration\n",
      "Loss=1.454060 at 676th iteration\n",
      "Loss=1.451774 at 677th iteration\n",
      "Loss=1.449498 at 678th iteration\n",
      "Loss=1.447230 at 679th iteration\n",
      "Loss=1.444972 at 680th iteration\n",
      "Loss=1.442722 at 681th iteration\n",
      "Loss=1.440482 at 682th iteration\n",
      "Loss=1.438250 at 683th iteration\n",
      "Loss=1.436028 at 684th iteration\n",
      "Loss=1.433814 at 685th iteration\n",
      "Loss=1.431609 at 686th iteration\n",
      "Loss=1.429413 at 687th iteration\n",
      "Loss=1.427226 at 688th iteration\n",
      "Loss=1.425048 at 689th iteration\n",
      "Loss=1.422878 at 690th iteration\n",
      "Loss=1.420717 at 691th iteration\n",
      "Loss=1.418564 at 692th iteration\n",
      "Loss=1.416420 at 693th iteration\n",
      "Loss=1.414285 at 694th iteration\n",
      "Loss=1.412158 at 695th iteration\n",
      "Loss=1.410040 at 696th iteration\n",
      "Loss=1.407930 at 697th iteration\n",
      "Loss=1.405829 at 698th iteration\n",
      "Loss=1.403736 at 699th iteration\n",
      "Loss=1.401651 at 700th iteration\n",
      "Loss=1.399575 at 701th iteration\n",
      "Loss=1.397507 at 702th iteration\n",
      "Loss=1.395447 at 703th iteration\n",
      "Loss=1.393395 at 704th iteration\n",
      "Loss=1.391352 at 705th iteration\n",
      "Loss=1.389317 at 706th iteration\n",
      "Loss=1.387290 at 707th iteration\n",
      "Loss=1.385271 at 708th iteration\n",
      "Loss=1.383260 at 709th iteration\n",
      "Loss=1.381257 at 710th iteration\n",
      "Loss=1.379262 at 711th iteration\n",
      "Loss=1.377276 at 712th iteration\n",
      "Loss=1.375297 at 713th iteration\n",
      "Loss=1.373326 at 714th iteration\n",
      "Loss=1.371363 at 715th iteration\n",
      "Loss=1.369407 at 716th iteration\n",
      "Loss=1.367460 at 717th iteration\n",
      "Loss=1.365520 at 718th iteration\n",
      "Loss=1.363588 at 719th iteration\n",
      "Loss=1.361664 at 720th iteration\n",
      "Loss=1.359748 at 721th iteration\n",
      "Loss=1.357839 at 722th iteration\n",
      "Loss=1.355937 at 723th iteration\n",
      "Loss=1.354044 at 724th iteration\n",
      "Loss=1.352158 at 725th iteration\n",
      "Loss=1.350279 at 726th iteration\n",
      "Loss=1.348408 at 727th iteration\n",
      "Loss=1.346545 at 728th iteration\n",
      "Loss=1.344689 at 729th iteration\n",
      "Loss=1.342840 at 730th iteration\n",
      "Loss=1.340999 at 731th iteration\n",
      "Loss=1.339165 at 732th iteration\n",
      "Loss=1.337338 at 733th iteration\n",
      "Loss=1.335519 at 734th iteration\n",
      "Loss=1.333707 at 735th iteration\n",
      "Loss=1.331902 at 736th iteration\n",
      "Loss=1.330104 at 737th iteration\n",
      "Loss=1.328314 at 738th iteration\n",
      "Loss=1.326531 at 739th iteration\n",
      "Loss=1.324754 at 740th iteration\n",
      "Loss=1.322985 at 741th iteration\n",
      "Loss=1.321223 at 742th iteration\n",
      "Loss=1.319469 at 743th iteration\n",
      "Loss=1.317721 at 744th iteration\n",
      "Loss=1.315980 at 745th iteration\n",
      "Loss=1.314246 at 746th iteration\n",
      "Loss=1.312519 at 747th iteration\n",
      "Loss=1.310799 at 748th iteration\n",
      "Loss=1.309085 at 749th iteration\n",
      "Loss=1.307379 at 750th iteration\n",
      "Loss=1.305679 at 751th iteration\n",
      "Loss=1.303986 at 752th iteration\n",
      "Loss=1.302300 at 753th iteration\n",
      "Loss=1.300621 at 754th iteration\n",
      "Loss=1.298949 at 755th iteration\n",
      "Loss=1.297283 at 756th iteration\n",
      "Loss=1.295623 at 757th iteration\n",
      "Loss=1.293971 at 758th iteration\n",
      "Loss=1.292325 at 759th iteration\n",
      "Loss=1.290685 at 760th iteration\n",
      "Loss=1.289052 at 761th iteration\n",
      "Loss=1.287426 at 762th iteration\n",
      "Loss=1.285806 at 763th iteration\n",
      "Loss=1.284193 at 764th iteration\n",
      "Loss=1.282586 at 765th iteration\n",
      "Loss=1.280985 at 766th iteration\n",
      "Loss=1.279391 at 767th iteration\n",
      "Loss=1.277803 at 768th iteration\n",
      "Loss=1.276222 at 769th iteration\n",
      "Loss=1.274647 at 770th iteration\n",
      "Loss=1.273078 at 771th iteration\n",
      "Loss=1.271516 at 772th iteration\n",
      "Loss=1.269959 at 773th iteration\n",
      "Loss=1.268409 at 774th iteration\n",
      "Loss=1.266865 at 775th iteration\n",
      "Loss=1.265328 at 776th iteration\n",
      "Loss=1.263796 at 777th iteration\n",
      "Loss=1.262271 at 778th iteration\n",
      "Loss=1.260751 at 779th iteration\n",
      "Loss=1.259238 at 780th iteration\n",
      "Loss=1.257731 at 781th iteration\n",
      "Loss=1.256230 at 782th iteration\n",
      "Loss=1.254734 at 783th iteration\n",
      "Loss=1.253245 at 784th iteration\n",
      "Loss=1.251762 at 785th iteration\n",
      "Loss=1.250285 at 786th iteration\n",
      "Loss=1.248813 at 787th iteration\n",
      "Loss=1.247348 at 788th iteration\n",
      "Loss=1.245888 at 789th iteration\n",
      "Loss=1.244434 at 790th iteration\n",
      "Loss=1.242986 at 791th iteration\n",
      "Loss=1.241544 at 792th iteration\n",
      "Loss=1.240107 at 793th iteration\n",
      "Loss=1.238676 at 794th iteration\n",
      "Loss=1.237251 at 795th iteration\n",
      "Loss=1.235832 at 796th iteration\n",
      "Loss=1.234418 at 797th iteration\n",
      "Loss=1.233010 at 798th iteration\n",
      "Loss=1.231608 at 799th iteration\n",
      "Loss=1.230211 at 800th iteration\n",
      "Loss=1.228820 at 801th iteration\n",
      "Loss=1.227434 at 802th iteration\n",
      "Loss=1.226054 at 803th iteration\n",
      "Loss=1.224679 at 804th iteration\n",
      "Loss=1.223310 at 805th iteration\n",
      "Loss=1.221946 at 806th iteration\n",
      "Loss=1.220588 at 807th iteration\n",
      "Loss=1.219235 at 808th iteration\n",
      "Loss=1.217888 at 809th iteration\n",
      "Loss=1.216546 at 810th iteration\n",
      "Loss=1.215209 at 811th iteration\n",
      "Loss=1.213878 at 812th iteration\n",
      "Loss=1.212552 at 813th iteration\n",
      "Loss=1.211232 at 814th iteration\n",
      "Loss=1.209916 at 815th iteration\n",
      "Loss=1.208606 at 816th iteration\n",
      "Loss=1.207301 at 817th iteration\n",
      "Loss=1.206001 at 818th iteration\n",
      "Loss=1.204707 at 819th iteration\n",
      "Loss=1.203418 at 820th iteration\n",
      "Loss=1.202133 at 821th iteration\n",
      "Loss=1.200854 at 822th iteration\n",
      "Loss=1.199580 at 823th iteration\n",
      "Loss=1.198312 at 824th iteration\n",
      "Loss=1.197048 at 825th iteration\n",
      "Loss=1.195789 at 826th iteration\n",
      "Loss=1.194535 at 827th iteration\n",
      "Loss=1.193287 at 828th iteration\n",
      "Loss=1.192043 at 829th iteration\n",
      "Loss=1.190804 at 830th iteration\n",
      "Loss=1.189571 at 831th iteration\n",
      "Loss=1.188342 at 832th iteration\n",
      "Loss=1.187118 at 833th iteration\n",
      "Loss=1.185899 at 834th iteration\n",
      "Loss=1.184685 at 835th iteration\n",
      "Loss=1.183475 at 836th iteration\n",
      "Loss=1.182271 at 837th iteration\n",
      "Loss=1.181071 at 838th iteration\n",
      "Loss=1.179876 at 839th iteration\n",
      "Loss=1.178686 at 840th iteration\n",
      "Loss=1.177501 at 841th iteration\n",
      "Loss=1.176320 at 842th iteration\n",
      "Loss=1.175144 at 843th iteration\n",
      "Loss=1.173973 at 844th iteration\n",
      "Loss=1.172807 at 845th iteration\n",
      "Loss=1.171645 at 846th iteration\n",
      "Loss=1.170488 at 847th iteration\n",
      "Loss=1.169335 at 848th iteration\n",
      "Loss=1.168187 at 849th iteration\n",
      "Loss=1.167044 at 850th iteration\n",
      "Loss=1.165905 at 851th iteration\n",
      "Loss=1.164771 at 852th iteration\n",
      "Loss=1.163641 at 853th iteration\n",
      "Loss=1.162516 at 854th iteration\n",
      "Loss=1.161395 at 855th iteration\n",
      "Loss=1.160279 at 856th iteration\n",
      "Loss=1.159167 at 857th iteration\n",
      "Loss=1.158060 at 858th iteration\n",
      "Loss=1.156957 at 859th iteration\n",
      "Loss=1.155858 at 860th iteration\n",
      "Loss=1.154764 at 861th iteration\n",
      "Loss=1.153674 at 862th iteration\n",
      "Loss=1.152589 at 863th iteration\n",
      "Loss=1.151508 at 864th iteration\n",
      "Loss=1.150431 at 865th iteration\n",
      "Loss=1.149359 at 866th iteration\n",
      "Loss=1.148291 at 867th iteration\n",
      "Loss=1.147227 at 868th iteration\n",
      "Loss=1.146167 at 869th iteration\n",
      "Loss=1.145112 at 870th iteration\n",
      "Loss=1.144060 at 871th iteration\n",
      "Loss=1.143013 at 872th iteration\n",
      "Loss=1.141971 at 873th iteration\n",
      "Loss=1.140932 at 874th iteration\n",
      "Loss=1.139898 at 875th iteration\n",
      "Loss=1.138867 at 876th iteration\n",
      "Loss=1.137841 at 877th iteration\n",
      "Loss=1.136819 at 878th iteration\n",
      "Loss=1.135801 at 879th iteration\n",
      "Loss=1.134787 at 880th iteration\n",
      "Loss=1.133777 at 881th iteration\n",
      "Loss=1.132771 at 882th iteration\n",
      "Loss=1.131769 at 883th iteration\n",
      "Loss=1.130771 at 884th iteration\n",
      "Loss=1.129777 at 885th iteration\n",
      "Loss=1.128788 at 886th iteration\n",
      "Loss=1.127802 at 887th iteration\n",
      "Loss=1.126820 at 888th iteration\n",
      "Loss=1.125842 at 889th iteration\n",
      "Loss=1.124867 at 890th iteration\n",
      "Loss=1.123897 at 891th iteration\n",
      "Loss=1.122931 at 892th iteration\n",
      "Loss=1.121968 at 893th iteration\n",
      "Loss=1.121009 at 894th iteration\n",
      "Loss=1.120055 at 895th iteration\n",
      "Loss=1.119104 at 896th iteration\n",
      "Loss=1.118156 at 897th iteration\n",
      "Loss=1.117213 at 898th iteration\n",
      "Loss=1.116273 at 899th iteration\n",
      "Loss=1.115337 at 900th iteration\n",
      "Loss=1.114405 at 901th iteration\n",
      "Loss=1.113477 at 902th iteration\n",
      "Loss=1.112552 at 903th iteration\n",
      "Loss=1.111631 at 904th iteration\n",
      "Loss=1.110713 at 905th iteration\n",
      "Loss=1.109800 at 906th iteration\n",
      "Loss=1.108889 at 907th iteration\n",
      "Loss=1.107983 at 908th iteration\n",
      "Loss=1.107080 at 909th iteration\n",
      "Loss=1.106181 at 910th iteration\n",
      "Loss=1.105285 at 911th iteration\n",
      "Loss=1.104393 at 912th iteration\n",
      "Loss=1.103505 at 913th iteration\n",
      "Loss=1.102620 at 914th iteration\n",
      "Loss=1.101739 at 915th iteration\n",
      "Loss=1.100861 at 916th iteration\n",
      "Loss=1.099986 at 917th iteration\n",
      "Loss=1.099116 at 918th iteration\n",
      "Loss=1.098248 at 919th iteration\n",
      "Loss=1.097384 at 920th iteration\n",
      "Loss=1.096524 at 921th iteration\n",
      "Loss=1.095667 at 922th iteration\n",
      "Loss=1.094813 at 923th iteration\n",
      "Loss=1.093963 at 924th iteration\n",
      "Loss=1.093116 at 925th iteration\n",
      "Loss=1.092273 at 926th iteration\n",
      "Loss=1.091433 at 927th iteration\n",
      "Loss=1.090596 at 928th iteration\n",
      "Loss=1.089763 at 929th iteration\n",
      "Loss=1.088933 at 930th iteration\n",
      "Loss=1.088106 at 931th iteration\n",
      "Loss=1.087283 at 932th iteration\n",
      "Loss=1.086463 at 933th iteration\n",
      "Loss=1.085646 at 934th iteration\n",
      "Loss=1.084832 at 935th iteration\n",
      "Loss=1.084022 at 936th iteration\n",
      "Loss=1.083215 at 937th iteration\n",
      "Loss=1.082411 at 938th iteration\n",
      "Loss=1.081610 at 939th iteration\n",
      "Loss=1.080813 at 940th iteration\n",
      "Loss=1.080019 at 941th iteration\n",
      "Loss=1.079228 at 942th iteration\n",
      "Loss=1.078440 at 943th iteration\n",
      "Loss=1.077655 at 944th iteration\n",
      "Loss=1.076873 at 945th iteration\n",
      "Loss=1.076095 at 946th iteration\n",
      "Loss=1.075320 at 947th iteration\n",
      "Loss=1.074547 at 948th iteration\n",
      "Loss=1.073778 at 949th iteration\n",
      "Loss=1.073012 at 950th iteration\n",
      "Loss=1.072249 at 951th iteration\n",
      "Loss=1.071489 at 952th iteration\n",
      "Loss=1.070732 at 953th iteration\n",
      "Loss=1.069978 at 954th iteration\n",
      "Loss=1.069227 at 955th iteration\n",
      "Loss=1.068479 at 956th iteration\n",
      "Loss=1.067734 at 957th iteration\n",
      "Loss=1.066992 at 958th iteration\n",
      "Loss=1.066253 at 959th iteration\n",
      "Loss=1.065517 at 960th iteration\n",
      "Loss=1.064784 at 961th iteration\n",
      "Loss=1.064054 at 962th iteration\n",
      "Loss=1.063326 at 963th iteration\n",
      "Loss=1.062602 at 964th iteration\n",
      "Loss=1.061881 at 965th iteration\n",
      "Loss=1.061162 at 966th iteration\n",
      "Loss=1.060446 at 967th iteration\n",
      "Loss=1.059733 at 968th iteration\n",
      "Loss=1.059023 at 969th iteration\n",
      "Loss=1.058316 at 970th iteration\n",
      "Loss=1.057612 at 971th iteration\n",
      "Loss=1.056910 at 972th iteration\n",
      "Loss=1.056212 at 973th iteration\n",
      "Loss=1.055516 at 974th iteration\n",
      "Loss=1.054823 at 975th iteration\n",
      "Loss=1.054132 at 976th iteration\n",
      "Loss=1.053445 at 977th iteration\n",
      "Loss=1.052760 at 978th iteration\n",
      "Loss=1.052078 at 979th iteration\n",
      "Loss=1.051398 at 980th iteration\n",
      "Loss=1.050721 at 981th iteration\n",
      "Loss=1.050047 at 982th iteration\n",
      "Loss=1.049376 at 983th iteration\n",
      "Loss=1.048708 at 984th iteration\n",
      "Loss=1.048042 at 985th iteration\n",
      "Loss=1.047378 at 986th iteration\n",
      "Loss=1.046718 at 987th iteration\n",
      "Loss=1.046060 at 988th iteration\n",
      "Loss=1.045404 at 989th iteration\n",
      "Loss=1.044752 at 990th iteration\n",
      "Loss=1.044101 at 991th iteration\n",
      "Loss=1.043454 at 992th iteration\n",
      "Loss=1.042809 at 993th iteration\n",
      "Loss=1.042167 at 994th iteration\n",
      "Loss=1.041527 at 995th iteration\n",
      "Loss=1.040890 at 996th iteration\n",
      "Loss=1.040255 at 997th iteration\n",
      "Loss=1.039623 at 998th iteration\n",
      "Loss=1.038993 at 999th iteration\n"
     ]
    }
   ],
   "source": [
    "X, Y, m_orig, c_orig = sample()\n",
    "theta = {'m': 0, 'c': 0}\n",
    "i = 0\n",
    "while i<1000:\n",
    "    Y_hat, loss = forward_prop(theta, X, Y)\n",
    "    dtheta = back_prop(X, Y_hat, Y)\n",
    "    theta = update(theta, dtheta)\n",
    "    print('Loss=%f at %ith iteration' %(loss, i))\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
